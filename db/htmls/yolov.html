<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yolov</title>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Goldman&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/public/css/markdown.css">
    <link rel="stylesheet" href="/public/css/global.css">
    <link rel="stylesheet" href="/public/css/article.css">
</head>
<body>
    <header>
        <a href="/" id="logo-text"><span>PRODUCE.X</span></a>
        <img src="/public/image/me.jpg" alt="logo">
    </header>
    <div class="markdown-body">
        <a href="https://apps.apple.com/app/id1452689527" target="_blank">
<img src="https://user-images.githubusercontent.com/26833433/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg" width="1000"></a>
&nbsp

<p><img src="https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg" alt="CI CPU testing"></p>
<p>This repository represents Ultralytics open-source research into future object detection methods, and incorporates lessons learned and best practices evolved over thousands of hours of training and evolution on anonymized client datasets. <strong>All code and models are under active development, and are subject to modification or deletion without notice.</strong> Use at your own risk.</p>
<p><img src="https://user-images.githubusercontent.com/26833433/103594689-455e0e00-4eae-11eb-9cdf-7d753e2ceeeb.png" width="1000">** GPU Speed measures end-to-end time per image averaged over 5000 COCO val2017 images using a V100 GPU with batch size 32, and includes image preprocessing, PyTorch FP16 inference, postprocessing and NMS. EfficientDet data from <a href="https://github.com/google/automl">google/automl</a> at batch size 8.</p>
<ul>
<li><strong>January 5, 2021</strong>: <a href="https://github.com/ultralytics/yolov5/releases/tag/v4.0">v4.0 release</a>: nn.SiLU() activations, <a href="https://wandb.ai/">Weights &amp; Biases</a> logging, <a href="https://pytorch.org/hub/ultralytics_yolov5/">PyTorch Hub</a> integration.</li>
<li><strong>August 13, 2020</strong>: <a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">v3.0 release</a>: nn.Hardswish() activations, data autodownload, native AMP.</li>
<li><strong>July 23, 2020</strong>: <a href="https://github.com/ultralytics/yolov5/releases/tag/v2.0">v2.0 release</a>: improved model definition, training and mAP.</li>
<li><strong>June 22, 2020</strong>: <a href="https://arxiv.org/abs/1803.01534">PANet</a> updates: new heads, reduced parameters, improved speed and mAP <a href="https://github.com/ultralytics/yolov5/commit/364fcfd7dba53f46edd4f04c037a039c0a287972">364fcfd</a>.</li>
<li><strong>June 19, 2020</strong>: <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.half">FP16</a> as new default for smaller checkpoints and faster inference <a href="https://github.com/ultralytics/yolov5/commit/d4c6674c98e19df4c40e33a777610a18d1961145">d4c6674</a>.</li>
</ul>
<h2 id="pretrained-checkpoints">Pretrained Checkpoints</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>size</th>
<th>AP<sup>val</sup></th>
<th>AP<sup>test</sup></th>
<th>AP<sub>50</sub></th>
<th>Speed<sub>V100</sub></th>
<th>FPS<sub>V100</sub></th>
<th></th>
<th>params</th>
<th align="center">GFLOPS</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5s</a></td>
<td>640</td>
<td>36.8</td>
<td>36.8</td>
<td>55.6</td>
<td><strong>2.2ms</strong></td>
<td><strong>455</strong></td>
<td></td>
<td>7.3M</td>
<td align="center">17.0</td>
</tr>
<tr>
<td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5m</a></td>
<td>640</td>
<td>44.5</td>
<td>44.5</td>
<td>63.1</td>
<td>2.9ms</td>
<td>345</td>
<td></td>
<td>21.4M</td>
<td align="center">51.3</td>
</tr>
<tr>
<td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5l</a></td>
<td>640</td>
<td>48.1</td>
<td>48.1</td>
<td>66.4</td>
<td>3.8ms</td>
<td>264</td>
<td></td>
<td>47.0M</td>
<td align="center">115.4</td>
</tr>
<tr>
<td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5x</a></td>
<td>640</td>
<td><strong>50.1</strong></td>
<td><strong>50.1</strong></td>
<td><strong>68.7</strong></td>
<td>6.0ms</td>
<td>167</td>
<td></td>
<td>87.7M</td>
<td align="center">218.8</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
</tr>
<tr>
<td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5x</a> + TTA</td>
<td>832</td>
<td><strong>51.9</strong></td>
<td><strong>51.9</strong></td>
<td><strong>69.6</strong></td>
<td>24.9ms</td>
<td>40</td>
<td></td>
<td>87.7M</td>
<td align="center">1005.3</td>
</tr>
</tbody></table>
<!--- 
| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases)   |640 |49.0     |49.0     |67.4     |4.1ms     |244     ||77.2M  |117.7
| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases)   |1280 |53.0     |53.0     |70.8     |12.3ms     |81     ||77.2M  |117.7
--->

<p>** AP<sup>test</sup> denotes COCO <a href="http://cocodataset.org/#upload">test-dev2017</a> server results, all other AP results denote val2017 accuracy.<br>** All AP numbers are for single-model single-scale without ensemble or TTA. <strong>Reproduce mAP</strong> by <code>python test.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65</code><br>** Speed<sub>GPU</sub> averaged over 5000 COCO val2017 images using a GCP <a href="https://cloud.google.com/compute/docs/machine-types#n1_standard_machine_types">n1-standard-16</a> V100 instance, and includes image preprocessing, FP16 inference, postprocessing and NMS. NMS is 1-2ms/img.  <strong>Reproduce speed</strong> by <code>python test.py --data coco.yaml --img 640 --conf 0.25 --iou 0.45</code><br>** All checkpoints are trained to 300 epochs with default settings and hyperparameters (no autoaugmentation). 
** Test Time Augmentation (<a href="https://github.com/ultralytics/yolov5/issues/303">TTA</a>) runs at 3 image sizes. <strong>Reproduce TTA</strong> by <code>python test.py --data coco.yaml --img 832 --iou 0.65 --augment</code> </p>
<h2 id="requirements">Requirements</h2>
<p>Python 3.8 or later with all <a href="https://github.com/ultralytics/yolov5/blob/master/requirements.txt">requirements.txt</a> dependencies installed, including <code>torch&gt;=1.7</code>. To install run:</p>
<pre><code class="language-bash">$ pip install -r requirements.txt</code></pre>
<h2 id="tutorials">Tutorials</h2>
<ul>
<li><a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train Custom Data</a>&nbsp; üöÄ RECOMMENDED</li>
<li><a href="https://github.com/ultralytics/yolov5/issues/1289">Weights &amp; Biases Logging</a>&nbsp; üåü NEW</li>
<li><a href="https://github.com/ultralytics/yolov5/issues/475">Multi-GPU Training</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/36">PyTorch Hub</a>&nbsp; ‚≠ê NEW</li>
<li><a href="https://github.com/ultralytics/yolov5/issues/251">ONNX and TorchScript Export</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/303">Test-Time Augmentation (TTA)</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/318">Model Ensembling</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/304">Model Pruning/Sparsity</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/607">Hyperparameter Evolution</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/1314">Transfer Learning with Frozen Layers</a>&nbsp; ‚≠ê NEW</li>
<li><a href="https://github.com/wang-xinyu/tensorrtx">TensorRT Deployment</a></li>
</ul>
<h2 id="environments">Environments</h2>
<p>YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including <a href="https://developer.nvidia.com/cuda">CUDA</a>/<a href="https://developer.nvidia.com/cudnn">CUDNN</a>, <a href="https://www.python.org/">Python</a> and <a href="https://pytorch.org/">PyTorch</a> preinstalled):</p>
<ul>
<li><strong>Google Colab Notebook</strong> with free GPU: <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></li>
<li><strong>Kaggle Notebook</strong> with free GPU: <a href="https://www.kaggle.com/ultralytics/yolov5">https://www.kaggle.com/ultralytics/yolov5</a></li>
<li><strong>Google Cloud</strong> Deep Learning VM. See <a href="https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart">GCP Quickstart Guide</a> </li>
<li><strong>Docker Image</strong> <a href="https://hub.docker.com/r/ultralytics/yolov5">https://hub.docker.com/r/ultralytics/yolov5</a>. See <a href="https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart">Docker Quickstart Guide</a> <img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></li>
</ul>
<h2 id="inference">Inference</h2>
<p>detect.py runs inference on a variety of sources, downloading models automatically from the <a href="https://github.com/ultralytics/yolov5/releases">latest YOLOv5 release</a> and saving results to <code>runs/detect</code>.</p>
<pre><code class="language-bash">$ python detect.py --source 0  # webcam
                            file.jpg  # image 
                            file.mp4  # video
                            path/  # directory
                            path/*.jpg  # glob
                            rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa  # rtsp stream
                            rtmp://192.168.1.105/live/test  # rtmp stream
                            http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8  # http stream</code></pre>
<p>To run inference on example images in <code>data/images</code>:</p>
<pre><code class="language-bash">$ python detect.py --source data/images --weights yolov5s.pt --conf 0.25

Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device=&#39;&#39;, img_size=640, iou_thres=0.45, save_conf=False, save_dir=&#39;runs/detect&#39;, save_txt=False, source=&#39;data/images/&#39;, update=False, view_img=False, weights=[&#39;yolov5s.pt&#39;])
Using torch 1.7.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16130MB)

Downloading https://github.com/ultralytics/yolov5/releases/download/v3.1/yolov5s.pt to yolov5s.pt... 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.5M/14.5M [00:00&lt;00:00, 21.3MB/s]

Fusing layers... 
Model Summary: 232 layers, 7459581 parameters, 0 gradients
image 1/2 data/images/bus.jpg: 640x480 4 persons, 1 buss, 1 skateboards, Done. (0.012s)
image 2/2 data/images/zidane.jpg: 384x640 2 persons, 2 ties, Done. (0.012s)
Results saved to runs/detect/exp
Done. (0.113s)</code></pre>
<img src="https://user-images.githubusercontent.com/26833433/97107365-685a8d80-16c7-11eb-8c2e-83aac701d8b9.jpeg" width="500">  

<h3 id="pytorch-hub">PyTorch Hub</h3>
<p>To run <strong>batched inference</strong> with YOLOv5 and <a href="https://github.com/ultralytics/yolov5/issues/36">PyTorch Hub</a>:</p>
<pre><code class="language-python">import torch
from PIL import Image

# Model
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, pretrained=True)

# Images
img1 = Image.open(&#39;zidane.jpg&#39;)
img2 = Image.open(&#39;bus.jpg&#39;)
imgs = [img1, img2]  # batched list of images

# Inference
result = model(imgs)</code></pre>
<h2 id="training">Training</h2>
<p>Run commands below to reproduce results on <a href="https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh">COCO</a> dataset (dataset auto-downloads on first use). Training times for YOLOv5s/m/l/x are 2/4/6/8 days on a single V100 (multi-GPU times faster). Use the largest <code>--batch-size</code> your GPU allows (batch sizes shown for 16 GB devices).</p>
<pre><code class="language-bash">$ python train.py --data coco.yaml --cfg yolov5s.yaml --weights &#39;&#39; --batch-size 64
                                         yolov5m                                40
                                         yolov5l                                24
                                         yolov5x                                16</code></pre>
<img src="https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png" width="900">


<h2 id="citation">Citation</h2>
<p><a href="https://zenodo.org/badge/latestdoi/264818686"><img src="https://zenodo.org/badge/264818686.svg" alt="DOI"></a></p>
<h2 id="about-us">About Us</h2>
<p>Ultralytics is a U.S.-based particle physics and AI startup with over 6 years of expertise supporting government, academic and business clients. We offer a wide range of vision AI services, spanning from simple expert advice up to delivery of fully customized, end-to-end production solutions, including:</p>
<ul>
<li><strong>Cloud-based AI</strong> systems operating on <strong>hundreds of HD video streams in realtime.</strong></li>
<li><strong>Edge AI</strong> integrated into custom iOS and Android apps for realtime <strong>30 FPS video inference.</strong></li>
<li><strong>Custom data training</strong>, hyperparameter evolution, and model exportation to any destination.</li>
</ul>
<p>For business inquiries and professional support requests please visit us at <a href="https://www.ultralytics.com">https://www.ultralytics.com</a>. </p>
<h2 id="contact">Contact</h2>
<p><strong>Issues should be raised directly in the repository.</strong> For business inquiries or professional support requests please visit <a href="https://www.ultralytics.com">https://www.ultralytics.com</a> or email Glenn Jocher at <a href="mailto:&#103;&#108;&#x65;&#110;&#110;&#x2e;&#106;&#111;&#99;&#104;&#101;&#114;&#64;&#x75;&#108;&#x74;&#114;&#x61;&#x6c;&#121;&#x74;&#105;&#x63;&#x73;&#x2e;&#99;&#x6f;&#109;">&#103;&#108;&#x65;&#110;&#110;&#x2e;&#106;&#111;&#99;&#104;&#101;&#114;&#64;&#x75;&#108;&#x74;&#114;&#x61;&#x6c;&#121;&#x74;&#105;&#x63;&#x73;&#x2e;&#99;&#x6f;&#109;</a>. </p>

    </div>
    <script src="/public/js/rem.js"></script>
</body>
</html>